{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAIS_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHyEkpkug4PgSbapdzDdk9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itsdawei/david-lee-twitter/blob/main/CAIS_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd /content/drive/MyDrive/CAIS++/Project/\n",
        "\n",
        "DATASET_DIR = './dataset/data.csv'\n",
        "EMBEDDINGS_DIR = './glove.6B.50d.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neHse_W4IyOJ",
        "outputId": "1f4cb261-bfd2-4aab-9451-b792e4cadcb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/CAIS++/Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "data = pd.read_csv(DATASET_DIR)\n",
        "valences = data['valence'].values\n",
        "authors = data['author'].values\n",
        "tweets = data['tweet'].values\n",
        "\n",
        "PERCENT_DATA = .05\n",
        "tweets_shuffled, labels_shuffled = shuffle(tweets, valences)\n",
        "\n",
        "total = int(len(tweets_shuffled) * PERCENT_DATA)\n",
        "\n",
        "tweets_shuffled = tweets_shuffled[:total]\n",
        "labels_shuffled = labels_shuffled[:total]"
      ],
      "metadata": {
        "id": "UfMZgbUu7IiY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "EMBEDDING_DIM = 50\n",
        "\n",
        "def load_valence_data(tweets, valences, embeddings_dir):\n",
        "\t# Load tweets, labels\n",
        "  print(\"1 -- Loading tweets and labels\")\n",
        "  labels = valences//4\n",
        "\n",
        "\n",
        "\t# Tokenize the tweets (convert sentence to sequence of words)\n",
        "  print(\"2 -- Tokenizing the tweets: converting sentences to sequence of words\")\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(tweets)\n",
        "  sequences = tokenizer.texts_to_sequences(tweets)\n",
        "  word_index = tokenizer.word_index\n",
        "\n",
        "\t# Pad sequences to ensure samples are the same size\n",
        "  print(\"3 -- Padding sequences to ensure samples are the same size\")\n",
        "  training_data = pad_sequences(sequences)\n",
        "  \n",
        "  print(\"4 -- Loading pre-trained word embeddings. This may take a few minutes.\")\n",
        "  embeddings_index = {}\n",
        "  f = open(embeddings_dir,'rb')\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0].decode('UTF-8')\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  f.close()\n",
        "\n",
        "  print(\"5 -- Finding word embeddings for words in our tweets.\")\n",
        "  # prepare word embedding matrix\n",
        "  num_words = len(word_index)+1\n",
        "  embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      if i >= num_words:\n",
        "          continue\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  return tweets, training_data, labels, word_index, embedding_matrix\n"
      ],
      "metadata": {
        "id": "WlNwYDNYxPG3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets, tweets_preprocessed, labels, word_index, embedding_matrix = load_valence_data(tweets_shuffled, labels_shuffled, EMBEDDINGS_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1lfD30DIOOk",
        "outputId": "f24e2efe-3d9b-45b8-be58-fb5562566746"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 -- Loading tweets and labels\n",
            "2 -- Tokenizing the tweets: converting sentences to sequence of words\n",
            "3 -- Padding sequences to ensure samples are the same size\n",
            "4 -- Loading pre-trained word embeddings. This may take a few minutes.\n",
            "5 -- Finding word embeddings for words in our tweets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(tweets_preprocessed, labels, train_size=0.8, test_size=0.2, shuffle=True, random_state=0)"
      ],
      "metadata": {
        "id": "9fo4MNTbvn3T"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Input\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.layers.core import Dense, Activation, Flatten\n",
        "from keras.layers import Dropout, concatenate\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from keras import metrics\n",
        "from keras.models import Model\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Add pre-trained embedding layer \n",
        "# converts word indices to GloVe word embedding vectors as they're fed in\n",
        "model.add(Embedding(len(word_index) + 1,\n",
        "                    EMBEDDING_DIM,\n",
        "                    weights=[embedding_matrix],\n",
        "                    input_length=tweets_preprocessed.shape[1],\n",
        "                    trainable=False))\n",
        "\n",
        "# First LSTM layer\n",
        "model.add(LSTM(64, return_sequences = True, activation='relu'))\n",
        "model.add(Dropout(.2))\n",
        "\n",
        "# Second LSTM layer \n",
        "model.add(LSTM(64, activation='relu'))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "LOSS = 'categorical_crossentropy'\n",
        "OPTIMIZER = 'RMSprop'\n",
        "\n",
        "model.compile(loss = LOSS, optimizer = OPTIMIZER, metrics = [metrics.binary_accuracy])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTfl2920QxsA",
        "outputId": "da580301-6ca4-45f6-ce8c-0675f3583f71"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_9 (Embedding)     (None, 39, 50)            6943300   \n",
            "                                                                 \n",
            " lstm_18 (LSTM)              (None, 39, 64)            29440     \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 39, 64)            0         \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 64)                33024     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,005,829\n",
            "Trainable params: 62,529\n",
            "Non-trainable params: 6,943,300\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_SIZE = 0.5\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "model.fit(x_train, y_train, \n",
        "          epochs = EPOCHS, \n",
        "          batch_size = BATCH_SIZE, \n",
        "          validation_split = TEST_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "MTx36DWsRVfc",
        "outputId": "833a29b7-dcc4-42c6-ccf2-1c751e736b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/c_api_util.py\u001b[0m in \u001b[0;36mtf_buffer\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    202\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0;32myield\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_attr\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2615\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mc_api_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2616\u001b[0;31m         \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_OperationGetAttrValueProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2617\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Operation 'sequential_10/lstm_19/while/Placeholder_1' has no attr named '_user_specified_name'.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: 'zmq.backend.cython.message.Frame.__dealloc__'\n",
            "Traceback (most recent call last):\n",
            "  File \"zmq/backend/cython/checkrc.pxd\", line 13, in zmq.backend.cython.checkrc._check_rc\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ]
    }
  ]
}